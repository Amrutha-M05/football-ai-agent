{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9e13f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Football AI Agent - Quick Start\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a quick introduction to training and evaluating football AI agents.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Setup\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add project root to path\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.agents.dqn_agent import DQNAgent\\n\",\n",
    "    \"from src.agents.ppo_agent import PPOAgent\\n\",\n",
    "    \"from src.environment.wrappers import create_football_env\\n\",\n",
    "    \"from src.utils.metrics import PerformanceTracker\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"PyTorch version: {torch.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"CUDA available: {torch.cuda.is_available()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set style\\n\",\n",
    "    \"sns.set_style('whitegrid')\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Create Environment\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create a simple football environment to start with.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create environment\\n\",\n",
    "    \"env = create_football_env(\\n\",\n",
    "    \"    scenario='academy_empty_goal',\\n\",\n",
    "    \"    frame_skip=4,\\n\",\n",
    "    \"    reward_shaping=True,\\n\",\n",
    "    \"    render=False\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"State dimension: {env.observation_space.shape[0]}\\\")\\n\",\n",
    "    \"print(f\\\"Action dimension: {env.action_space.n}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Create and Train Agent\\n\",\n",
    "    \"\\n\",\n",
    "    \"We'll train a PPO agent for a few episodes as a demonstration.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Agent configuration\\n\",\n",
    "    \"config = {\\n\",\n",
    "    \"    'learning_rate': 3e-4,\\n\",\n",
    "    \"    'gamma': 0.99,\\n\",\n",
    "    \"    'gae_lambda': 0.95,\\n\",\n",
    "    \"    'clip_epsilon': 0.2,\\n\",\n",
    "    \"    'value_coef': 0.5,\\n\",\n",
    "    \"    'entropy_coef': 0.01,\\n\",\n",
    "    \"    'rollout_length': 2048,\\n\",\n",
    "    \"    'batch_size': 64,\\n\",\n",
    "    \"    'update_epochs': 4,\\n\",\n",
    "    \"    'use_cuda': torch.cuda.is_available()\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create agent\\n\",\n",
    "    \"agent = PPOAgent(\\n\",\n",
    "    \"    state_dim=env.observation_space.shape[0],\\n\",\n",
    "    \"    action_dim=env.action_space.n,\\n\",\n",
    "    \"    config=config\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Agent created successfully!\\\")\\n\",\n",
    "    \"print(f\\\"Device: {agent.device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Training loop (simplified)\\n\",\n",
    "    \"num_episodes = 100  # Short demo\\n\",\n",
    "    \"tracker = PerformanceTracker()\\n\",\n",
    "    \"episode_rewards = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training for {num_episodes} episodes...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for episode in range(num_episodes):\\n\",\n",
    "    \"    state = env.reset()\\n\",\n",
    "    \"    episode_reward = 0\\n\",\n",
    "    \"    episode_steps = 0\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        # Select action\\n\",\n",
    "    \"        action = agent.select_action(state, training=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Take step\\n\",\n",
    "    \"        next_state, reward, done, info = env.step(action)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Store transition\\n\",\n",
    "    \"        agent.store_transition(state, action, reward, next_state, done)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Update\\n\",\n",
    "    \"        if len(agent.buffer) >= agent.rollout_length:\\n\",\n",
    "    \"            agent.update()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        state = next_state\\n\",\n",
    "    \"        episode_reward += reward\\n\",\n",
    "    \"        episode_steps += 1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Track performance\\n\",\n",
    "    \"    episode_rewards.append(episode_reward)\\n\",\n",
    "    \"    tracker.add_episode(episode_reward, episode_steps, info)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Progress\\n\",\n",
    "    \"    if (episode + 1) % 20 == 0:\\n\",\n",
    "    \"        recent_mean = np.mean(episode_rewards[-20:])\\n\",\n",
    "    \"        print(f\\\"Episode {episode + 1}/{num_episodes} - Recent Mean Reward: {recent_mean:.2f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nTraining completed!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Visualize Training Progress\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training curve\\n\",\n",
    "    \"plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"plt.plot(episode_rewards, alpha=0.6, label='Episode Reward')\\n\",\n",
    "    \"# Moving average\\n\",\n",
    "    \"window = 20\\n\",\n",
    "    \"if len(episode_rewards) >= window:\\n\",\n",
    "    \"    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\\n\",\n",
    "    \"    plt.plot(range(window-1, len(episode_rewards)), moving_avg, \\n\",\n",
    "    \"             linewidth=2, label=f'{window}-Episode Moving Average')\\n\",\n",
    "    \"plt.xlabel('Episode')\\n\",\n",
    "    \"plt.ylabel('Reward')\\n\",\n",
    "    \"plt.title('Training Progress')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"plt.hist(episode_rewards, bins=20, edgecolor='black', alpha=0.7)\\n\",\n",
    "    \"plt.xlabel('Episode Reward')\\n\",\n",
    "    \"plt.ylabel('Frequency')\\n\",\n",
    "    \"plt.title('Reward Distribution')\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Evaluate Agent\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate on 20 episodes\\n\",\n",
    "    \"eval_rewards = []\\n\",\n",
    "    \"eval_scores = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Evaluating agent...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for episode in range(20):\\n\",\n",
    "    \"    state = env.reset()\\n\",\n",
    "    \"    episode_reward = 0\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        action = agent.select_action(state, training=False)\\n\",\n",
    "    \"        state, reward, done, info = env.step(action)\\n\",\n",
    "    \"        episode_reward += reward\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    eval_rewards.append(episode_reward)\\n\",\n",
    "    \"    eval_scores.append(info.get('score_reward', 0))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print statistics\\n\",\n",
    "    \"print(f\\\"\\\\nEvaluation Results ({len(eval_rewards)} episodes):\\\")\\n\",\n",
    "    \"print(f\\\"  Mean Reward: {np.mean(eval_rewards):.2f} Â± {np.std(eval_rewards):.2f}\\\")\\n\",\n",
    "    \"print(f\\\"  Win Rate: {np.mean(np.array(eval_scores) > 0):.2%}\\\")\\n\",\n",
    "    \"print(f\\\"  Mean Score: {np.mean(eval_scores):.2f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Save Agent\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save trained agent\\n\",\n",
    "    \"save_path = '../checkpoints/notebook_demo_agent.pth'\\n\",\n",
    "    \"agent.save(save_path)\\n\",\n",
    "    \"print(f\\\"Agent saved to {save_path}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Next Steps\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Train for more episodes for better performance\\n\",\n",
    "    \"2. Try different scenarios (e.g., '11_vs_11_stochastic')\\n\",\n",
    "    \"3. Experiment with hybrid agents (PPO + LightGBM)\\n\",\n",
    "    \"4. Compare DQN vs PPO performance\\n\",\n",
    "    \"5. Benchmark CPU vs GPU training speed\\n\",\n",
    "    \"\\n\",\n",
    "    \"See the other notebooks for more advanced examples!\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
